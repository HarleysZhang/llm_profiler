{
    "opt-1.3b":{
        "num_layers": 24,
        "n_head": 32,
        "hidden_dim": 2048,
        "vocab_size": 50272,
        "max_seq_len": 2048,
        "ffn_embed_dim": 8192,
        "model_type": "opt",
        "model_name": "opt-1.3b"
    },
    "opt-6.7b":{
        "num_layers": 32,
        "n_head": 32,
        "hidden_dim": 4096,
        "vocab_size": 50272,
        "max_seq_len": 2048,
        "ffn_embed_dim": 16384,
        "model_type": "opt",
        "model_name": "opt-6.7b"
    },
    "opt-13b":{
        "num_layers": 40,
        "n_head": 40,
        "hidden_dim": 5120,
        "vocab_size": 50272,
        "max_seq_len": 2048,
        "ffn_embed_dim": 20480,
        "model_type": "opt",
        "model_name": "opt-13b"
    },
    "opt-66b":{
        "num_layers": 64,
        "n_head": 72,
        "hidden_dim": 9216,
        "vocab_size": 50272,
        "max_seq_len": 2048,
        "ffn_embed_dim": 36864,
        "model_type": "opt",
        "model_name": "opt-66b"
    },
    "opt-175b":{
        "max_seq_len": 2048,
        "num_layers": 96,
        "n_head": 96,
        "hidden_dim": 12288,
        "vocab_size": 50272,
        "ffn_embed_dim": 49152,
        "model_type": "opt",
        "model_name": "opt-175b"
    },
    "gpt2":{
        "num_layers": 12,
        "n_head": 12,
        "hidden_dim": 768,
        "vocab_size": 50257,
        "max_seq_len": 1024,
        "ffn_embed_dim": 3072,
        "model_type": "gpt2",
        "model_name": "gpt2"
    },
    "gpt2-medium":{
        "num_layers": 24,
        "n_head": 16,
        "hidden_dim": 1024,
        "vocab_size": 50257,
        "max_seq_len": 1024,
        "ffn_embed_dim": 4096,
        "model_type": "gpt2",
        "model_name": "gpt2-medium"
    },
    "gpt2-large":{
        "num_layers": 36,
        "n_head": 20,
        "hidden_dim": 1280,
        "vocab_size": 50257,
        "max_seq_len": 1024,
        "ffn_embed_dim": 5120,
        "model_type": "gpt2",
        "model_name": "gpt2-large"
    },
    "gpt2-xl":{
        "num_layers": 48,
        "n_head": 25,
        "hidden_dim": 1600,
        "vocab_size": 50257,
        "max_seq_len": 1024,
        "ffn_embed_dim": 6400,
        "model_type": "gpt2",
        "model_name": "gpt2-xl"
    },
    "bloom-560m":{
        "num_layers": 24,
        "n_head": 16,
        "hidden_dim": 1024,
        "vocab_size": 250880,
        "max_seq_len": null,
        "ffn_embed_dim": 4096,
        "model_type": "bloom",
        "model_name": "bloom-560m"
    },
    "bloom-7b":{
        "num_layers": 30,
        "n_head": 32,
        "hidden_dim": 4096,
        "vocab_size": 250880,
        "max_seq_len": null,
        "ffn_embed_dim": 16384,
        "model_type": "bloom",
        "model_name": "bloom-7b"
    },
    "bloom-175b":{
        "num_layers": 96,
        "n_head": 96,
        "hidden_dim": 12288,
        "vocab_size": 250880,
        "ffn_embed_dim": 49152,
        "model_type": "bloom",
        "model_name": "bloom-175b"
    },
    "llama-7b":{
        "num_layers": 32,
        "n_head": 32,
        "hidden_dim": 4096,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "ffn_embed_dim": 16384,
        "model_type": "llama"
    },
    "llama-13b":{
        "num_layers": 40,
        "n_head": 40,
        "hidden_dim": 5120,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "ffn_embed_dim": 20480,
        "model_type": "llama",
        "model_name": "llama-13b"
    },
    "llama-30b":{
        "num_layers": 60,
        "n_head": 52,
        "hidden_dim": 6656,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "ffn_embed_dim": 26624,
        "model_type": "llama",
        "model_name": "llama-30b"
    },
    "llama-65b":{
        "num_layers": 80,
        "n_head": 64,
        "hidden_dim": 8192,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "ffn_embed_dim": 32768,
        "model_type": "llama",
        "model_name": "llama-65b"
    }
}
